---
author: "Daniel Weinberger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Brazil example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
params:
  sensitivity: TRUE
  crossval: FALSE
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 3,
  fig.width = 5,
  fig.align = "center", 
  dpi=300, 
	out.width="600px"
)
```

---

```{r setup_packages, include=FALSE, echo=TRUE}
#Install the package
#library(devtools)
#devtools::install_github('https://github.com/weinbergerlab/InterventionEvaluatR',ref = "master") 
library(knitr)
library(plyr)
library(InterventionEvaluatR)
library(coda)
library(HDInterval)
library(lubridate)
library(pbapply)
library(parallel)
```

---
title: "Estimated change associated with the introduction of vaccine in Brazil"
---

---
## View dataset
We are using a dataset that has several different age groups. These represent monthly time series for a number of different diseases. The columns have counts per month of hospitalizations for the causes indicated by the range of ICD10 codes indicated in the header name. 

For instance J12_18 has the nuber of hospitalizations that had ICD10 codes in the range J12-J18 during that month in the indicated age group.  We also will subset the data (just take age groups 8 and 9. It is also *very* important to sort the data by age group and date.

Here we load the data and do some minor pre-processing to subset the dataset. We will just keep age group 9 (<12 month old kids) and age group 8 (80+ year old adults)

```{r viewdata, include=TRUE}
   sa1<-read.csv('CausesOfDeath_1999_2016_collapsed_4Apr19_sub_ana_race.csv')
   sa1$date<-as.Date(sa1$date, '%m/%d/%Y' ) 
   
   #Others that we need to exclude?
   sa1<-sa1[,-which((names(sa1) %in% c('PI','AllRes', 'denom')))]
   sa1$race<-as.character(sa1$race)
   sa1<-sa1[sa1$race!='Asian',]
   
   #Filter variables that have no variance
   sa1.spl<-split(sa1, sa1$race)
   variance.vars<-lapply(sa1.spl, function(x) apply(x[1:123,], 2, var)  )
      for(i in 1:length(sa1.spl)){
     sa1.spl[[i]]<-sa1.spl[[i]][,-which(variance.vars[[i]]==0)]
   }
   sa1<-rbind.fill(sa1.spl) #cobine back together
   
```

#Plot time series
```{r}
unique(sa1$race)
age_subset<-sa1[sa1$race=='White',]
plot(age_subset$Pneum, type='l', bty='l')
abline(v=123, col='gray', lty=2)

matplot(scale(log(age_subset[,4:20]+0.5)), type='l', bty='l')
abline(v=123, col='gray', lty=2)

```


# Intervention EvaluatR Package

## Set parameters for analysis

Here we need to set a few parameters. We Use the evaluatr.init() function to specify the name of the dataset, the date at which the vaccine is introduced, the date at which we want to begin evaluating the vaccine (typically 1-2 year after vaccine introduction). We also provide some information on the dataset, sch as whether the data are monthly or quarterly (n_seasons), the variable names for the grouping variable, the date variable, the outcome variable, and the denominator variable (if any).

```{r setup_data, echo=TRUE}

analysis <- evaluatr.init(
  country = "SAfrica", data = sa1,
  post_period_start = "2009-04-01", #First 'post-intervention' month is Jan 2012
  eval_period_start = "2011-04-01", #We ignore first 2 years of data to allow for vaccine ramp up
  eval_period_end = "2016-12-01", #The evaluation period lasts 2 years
  n_seasons = 12, #This is monthly data, so select 12
  year_def = "cal_year", # we are in southern hemisphere, so aggregate results by calendar year (Jan-Dec)
  group_name = "race",  #Strata categry name
  date_name = "date", #Date variable name
  outcome_name = "Pneum", #Outcome variable name
  denom_name = "J00_J99_excl_PI_bron" #Denominator variable name
)
set.seed(1)
```



## Run a simple analysis controlling for 1 control variable at a time

Before getting into more complicated analyses, we will first try to fit a simple Poisson regression model (with overdispersion) where we adjust for seasonality and 1 control variable at a time. this allows us to see how the use of different controls influences the results

The results are ordered by goodness of fit (based on AIC scores), with best fitting covariates on top.
```{r univariate, fig.width=3, fig.height=5}
 glmer_results= evaluatr.univariate(analysis)
 lapply(glmer_results,evaluatr.univariate.plot)
```

## Run the main analysis
Save the results in object 'impact_results'
```{r main analysis, include = FALSE}
impact_results = evaluatr.impact(analysis)
```


## Check model convergence
These models are fit using Markov Chain Monte Carlo (MCMC). It is important to evaluate the convergence of the model. A quick way to check this is to evaluate the trace plots for rate ratio estimates from the synthetic controls model (or for the other model variants). We use Geweke's diagnostic, which tests whether the mean estimate for the rate ratio in the first 10% of the iterations is equal to the mean estimate for the rate ration in the last 50% of the iterations. If the model has not converged, you might need to add more iterations or a longer burn in period
```{r}
all.traces<-sapply(impact_results,'[[', 'rr_iter')
cats<-dimnames(all.traces[[1]])[[1]]
all.converge.status<-sapply(impact_results,'[[', 'converge')
for(j in c('full','time','pca')){
for(i in 1: nrow(all.traces[[1]])){
   plot(all.traces[[j]][i,], type='l', main=paste0(j,' ',cats[i],' ' ,all.converge.status[[j]][i,2] ), bty='l', ylim=c(0.2,2))
}
}

```

#`r params$country` Results


```{r sparse, results="asis", echo=F}
if (!is.null(names(analysis$sparse_groups[analysis$sparse_groups])) && length(names(analysis$sparse_groups[analysis$sparse_groups])) != 0) {
  print(xtable(data.frame("Sparse Groups" = names(analysis$sparse_groups[analysis$sparse_groups]), check.names = FALSE), align = "cc"), type="html")
}
```

## Compare estimates from different models
This shows the estimated rate ratio and 95% credible intervals from a synthetic controls analysis; a time-trend analysis where we used the specified denominator (all non-respiratory deaths) to adjust the number of pneumonia deaths in each month and a linear trend for time; a classic interrupted time series analysis (segmented regression); and the STL+PCA approach, which smooths and combines the control variables prior to including them in the model. 
```{r Comparison of estimates from different models, results="asis"}
results.table<- cbind.data.frame(
  #impact_results$best$rr_mean_intervals, 
  impact_results$full$rr_mean_intervals, 
  impact_results$time$rr_mean_intervals, 
  #impact_results$time_no_offset$rr_mean_intervals, 
  impact_results$its$rr_mean_intervals, 
  impact_results$pca$rr_mean_intervals)

  table<-xtable(results.table)
  htmlTable(table)
```


## Cases averted
How many cases were prevented from the time of vaccine introduction to the last time point in each stratum (+/- 95% CrI)? You can modify the number 'last.point' to pull out the cumulative number of cases at any point in the time series. In this case we are printing results fromthe SC model
```{r}
last.point<-dim(impact_results$full$cumsum_prevented)[1]
cum.prevented<-impact_results$full$cumsum_prevented[last.point,,]
```

Format and print table 
```{r}
cum1<- round(t(cum.prevented))
cum2<- paste0(cum1[,'50%'], ' (', cum1[,'2.5%'],', ',cum1[,'97.5%'],')')
cum3<-cbind.data.frame(row.names(cum1), cum2)
names(cum3)<-c('Stratum','Cases Averted (95% CrI)')
  htmlTable(cum3, align='l')
```

## Number of variables selected in SC analysis
The Synthetic controls analysis uses a Bayesian variable selection algorithm to weight the candidate covariates. In each MCMC iteration, it tests a different combination of variables. The model size indicates how many variables are selected in any given model. If <1 variable is selected on average, this indicates that no suitable control variables were identified. 
In this example 1-2 variables were selected on average in the 2-23m and 2-59 m age categories, while no controls were identified in the 24-59m age group (the average model size is <1 (0.44)).
```{r modelsize, results="asis"}
model_size = data.frame(t(analysis$model_size))
htmlTable(model_size, align='c')
```

## Inclusion Probabilities
```{r incl, include = FALSE}
incl_probs <- NULL
for (group in analysis$groups) {
  incl_prob <- impact_results$full$groups[[group]]$inclusion_probs[-c(1:(analysis$n_seasons - 1)), ]
  incl_prob <- incl_prob[order(-incl_prob$inclusion_probs), ]
  incl_prob <- incl_prob[c(1:3), ]
  incl_prob2 <- round(incl_prob[, 2],2)
  incl_prob_names <- incl_prob[, 1]
  incl_prob3 <- data.frame("Group" = group, "Greatest Inclusion Variable" = incl_prob_names[1], "Greatest Inclusion Probability" = incl_prob2[1], "Second Greatest Inclusion Variable" = incl_prob_names[2], "Second Greatest Inclusion Probability" = incl_prob2[2], "Third Greatest Inclusion Variable" = incl_prob_names[3], "Third Greatest Inclusion Probability" = incl_prob2[3], check.names = FALSE)
  incl_probs <- rbind(incl_probs, incl_prob3)
}
rownames(incl_probs) <- NULL
```

Here we can see which variables received the most weight in the models for each strata. Values range from 0-1, with values closer to 1 indicating that the variable was included in a larger proportion of the variable combinations that were tested A value of 1 would indicate that the variable was always included in the model, a value of 0.48 would indicate the variable was included in 48% of the models that were tested.
```{r incl_table, results="asis"}
htmlTable(incl_probs)
```


##combine estimates
```{r Comparison of estimates from different models}
if (params$crossval) {
  htmlTable(cbind.data.frame(crossval_results$rr_mean_stack_intervals, impact_results$full$rr_mean_intervals, impact_results$time$rr_mean_intervals, impact_results$time_no_offset$rr_mean_intervals, impact_results$its$rr_mean_intervals, impact_results$pca$rr_mean_intervals), align = "c")
} else {
  htmlTable(cbind.data.frame(impact_results$best$rr_mean_intervals, impact_results$full$rr_mean_intervals, impact_results$time$rr_mean_intervals, impact_results$time_no_offset$rr_mean_intervals, impact_results$its$rr_mean_intervals, impact_results$pca$rr_mean_intervals), align = "c")
}

```

##Plot of Rate ratios, with size proportional to cross validation weights
```{r mainplot1, echo=FALSE}
plots <- evaluatr.plots(analysis)
plots$summary
```


## Weight Sensitivity Analysis
```{r sensitivity}
if (exists("sensitivity_results")) {
  htmlTable(sensitivity_results$sensitivity_table_intervals, align = "c")
}
```


## Plot Observed vs expected monthly time series
```{r plots, results = 'asis', plot.width=5, plot.height=12}
for (group in names(plots$groups)) {
      par(mfrow=c(4,1))
      print(plots$groups[[group]]$pred_full )
      print(plots$groups[[group]]$pred_best )
      print(plots$groups[[group]]$pred_time )
      print(plots$groups[[group]]$pred_pca )
}
```

## Plot Observed vs expected yearly time series
```{r plots2, results = 'asis', plot.width=5, plot.height=12}
for (group in names(plots$groups)) {
      par(mfrow=c(4,1))
      print(plots$groups[[group]]$pred_full_agg )
      print(plots$groups[[group]]$pred_best_agg )
      print(plots$groups[[group]]$pred_time_agg )
      print(plots$groups[[group]]$pred_pca_agg )
}
```

## Plot cumulative cases prevented
Estimated using the 'best' model, between SC and STL+PCA

```{r plots3, results = 'asis', plot.width=5, plot.height=12}
for (group in names(plots$groups)) {
      par(mfrow=c(4,1))
      print(plots$groups[[group]]$cumsum_prevented )
}
```

## Print results
```{r save_results, echo=FALSE}
output_file <- "Results" # Directory where results will be saved.
output_file <- paste0(output_file, "_", analysis$country, "_", format(Sys.time(), "%Y-%m-%d-%H%M%S"), ".Rds")
evaluatr.save(analysis, output_file)
```



##Horseshoe prior
```{r}

call.jags.mod<-function(ds.in){  
  exclude.cols<-c(analysis$group_name, analysis$outcome_name, analysis$date_name)
  x<-ds.in[,-which(names(ds.in) %in% exclude.cols)]
    
  #Filter columsn with 0 variations in the covariate in the pre-vax period
  x.var<-apply(x,2, function(xx) var(xx[ds.in[,analysis$date_name]<analysis$post_period[1]] ))  
  x<-x[,x.var>0] 
  
  x.scale<-apply(x,2, function(z) scale(log(z+0.5))) 
  y<-ds.in[,analysis$outcome_name] 
  ds2<-cbind.data.frame(y, x.scale) 
  names(ds2)<-c(analysis$outcome_name,names(x)) 
  ds2.pre<-ds2[ds.in[,analysis$date_name]<analysis$post_period[1] ,] 
  ds3<-ds2 
  ds3[,analysis$outcome_name][ds.in[,analysis$date_name]>=analysis$post_period[1]]<-NA 
  months<-month(ds.in[,analysis$date_name]) 
  month.mat<-dummies::dummy(months) 
  month.mat<-month.mat[,-1]
  month.mat<-cbind(rep(1,nrow(month.mat)), month.mat) #add intercept
  month.mat.pre<- month.mat[ds.in[,analysis$date_name]<analysis$post_period[1] ,]
  
  mod.txt<- source('../jags/Non_spatial-Non_lagged-independent.txt')
  mod1<-nonspace_nonlag(burnin=15000, 
                        samples=10000,
                        thin=1,
                        chains=1,
                        regularize=TRUE, 
                        dic=FALSE,
                        n_full=length(ds3[,analysis$outcome_name]),
                        n_modeling=nrow(ds2.pre),
                        y_modeling=ds3[,analysis$outcome_name],
                        offset=rep(0, length(y)) ,
                        z=month.mat,  #parameters not being shrunk
                        x=ds3[,-1]
                        )
  
  posterior_samples.all<-do.call(rbind,mod1[[1]])
  post_means<-apply(posterior_samples.all, 2, median)
  sample.labs<-names(post_means)
  ci<-t(hdi(posterior_samples.all, credMass = 0.95))
  row.names(ci)<-sample.labs
  names(post_means)<-sample.labs
  post.combo<-cbind(post_means,ci)
  
  post.combo.y<-post.combo[grep('Y_pred', dimnames(post.combo)[[1]]),]
  post.combo.beta<-cbind.data.frame(post.combo[grep('beta', dimnames(post.combo)[[1]]),],dimnames(x)[[2]])
  # plot(y=1:nrow(post.combo.beta), x=post.combo.beta[,'post_means'], xlim=range(post.combo.beta[,1:3]), bty='l')
  # arrows(y0=1:nrow(post.combo.beta) ,x0=post.combo.beta[,2], x1=post.combo.beta[,3], length=0)
  # abline(v=0, col='gray', lty=2)
  # text(y=1:nrow(post.combo.beta), x=post.combo.beta[,'post_means']+0.1,dimnames(x)[[2]] , col='gray')
  
  #plot for post-vax period
  # matplot(post.combo.y, type='l')
  #  points(ds1$sALRI[(nrow(ds2.pre)+1):nrow(ds1)])
  # 
    log.rr.pointwise<- log((ds.in[(nrow(ds2.pre)+1):nrow(ds.in),analysis$outcome_name]+0.5)/(post.combo.y+0.5))
   # matplot(log.rr, type='l', col='gray', lty=c(2,1,2))
   # abline(h=0, col='red')
   
   length.rollout<-round(as.numeric((analysis$eval_period[1]-analysis$post_period[1]  )/30.3))
   post.samples<-posterior_samples.all[,-c(1:length.rollout)]
   post.samples.y<- post.samples[,grep('Y_pred', dimnames(post.samples)[[2]])]
   post.samples.sum<-apply(post.samples,1,sum)
   obs.post.sum<- sum(ds2[,analysis$outcome_name][ds.in[,analysis$date_name]>=analysis$post_period[1]][-c(1:length.rollout)])
   rr.agg<-obs.post.sum/post.samples.sum
   rr.q<-quantile(rr.agg, probs=c(0.025, 0.5, 0.975))
   output.list<-list('rr.samples'=rr.agg,'rr.q'=rr.q,'log.rr.pointwise.q'=log.rr.pointwise,'betas'=post.combo.beta)
}

```

```{r}
n_cores<-detectCores() - 1
ds.age.spl<-split(analysis$input_data, analysis$input_data[,analysis$group_name])
  cl <- makeCluster(n_cores)
  clusterEvalQ(cl, {
    library(lubridate, quietly = TRUE)
    library(HDInterval, quietly = TRUE)
  })
  clusterExport(cl, c('call.jags.mod','analysis'), environment())
  
  mod1<-pblapply(cl = cl,X=ds.age.spl,FUN=call.jags.mod)
stopCluster(cl)

rr.summary<-t(sapply(mod1, '[[', 'rr.q'))
rr.pointwisesummary<-sapply(mod1, '[[', 'log.rr.pointwise.q',simplify='array')

betas<-sapply(mod1, '[[', 'betas',simplify=F)

betas<-lapply(betas, function(x) x[order(x$post_means),])
```

```{r}

```

